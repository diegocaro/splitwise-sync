{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744ca8f-f868-477b-ba8f-62b1aa631f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27b700-d737-4eba-a5c9-6d84026081b3",
   "metadata": {},
   "source": [
    "1. Obtener ubicaciones - cuando los pagos fueron realizados por Diego\n",
    "2. Obtener correos electrónicos de transacciones, a partir del 2025-01-01\n",
    "3. Obtener transacciones de Splitwise, incluir las transacciones eliminadas, pero mantener solo las más recientes con el mismo precio y descripción/hash.\n",
    "   Conservar solo las creadas por Diego.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06839de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datetime_serie(serie: pd.Series, timezone=\"America/Santiago\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cleans a datetime column by converting it to a standard format.\n",
    "    \"\"\"\n",
    "    new = pd.to_datetime(\n",
    "        serie, format=\"ISO8601\", utc=True, errors=\"coerce\"\n",
    "    ).dt.tz_convert(timezone)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed8c44-3350-4ab5-afbe-a7ba8a88e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_locations(file_name: str) -> gpd.GeoDataFrame:\n",
    "    assert file_name.endswith(\".jsonl\"), \"File must be a JSON Lines file\"\n",
    "    df = pd.read_json(file_name, lines=True)\n",
    "    df[\"date\"] = clean_datetime_serie(df[\"date\"])\n",
    "    df = df.reset_index(drop=False, names=[\"id\"])\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[\"lon\"], df[\"lat\"]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "    drop_columns = [\"lon\", \"lat\", \"transaction\"]\n",
    "    drop_columns = [col for col in drop_columns if col in df.columns]\n",
    "    gdf = gdf.drop(columns=drop_columns)\n",
    "\n",
    "    gdf = gdf.rename(columns=lambda x: f\"location_{x}\" if x not in [\"geometry\"] else x)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "locations = read_locations(\"raw/locations.jsonl\")\n",
    "locations.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea130228",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "EXPENSES_CLEANED_COLUMNS = [\n",
    "    \"id\",\n",
    "    \"date\",\n",
    "    \"category_name\",\n",
    "    \"description\",\n",
    "    \"cost\",\n",
    "    \"details\",\n",
    "    \"created_at\",\n",
    "    \"updated_at\",\n",
    "    \"deleted_at\",\n",
    "    \"created_by_name\",\n",
    "    \"updated_by_name\",\n",
    "    \"deleted_by_name\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_duplicated_expense(expenses: pd.DataFrame) -> pd.Series:\n",
    "    # Un expense se considera duplicado si tiene la misma fecha, costo y descripción\n",
    "    # Marcaremos como duplicados a los que tienen una fecha de deleted_at no nula (que fueron eliminados)\n",
    "    # cols = [\"date\", \"cost\", \"description\"]\n",
    "    cols = [\n",
    "        \"date\",\n",
    "        \"cost\",\n",
    "    ]  # I changed the description of some expenses when testing, but they are the same in the end...\n",
    "    duplicated = (\n",
    "        expenses.groupby(cols)\n",
    "        .agg(cnt=(\"id\", \"count\"), ids=(\"id\", list))\n",
    "        .pipe(lambda df: df[df.cnt > 1])\n",
    "    )\n",
    "    duplicated_ids = duplicated[\"ids\"].explode().unique()\n",
    "    is_duplicated = expenses[\"id\"].isin(duplicated_ids) & ~expenses[\"deleted_at\"].isna()\n",
    "    return is_duplicated\n",
    "\n",
    "\n",
    "def read_expenses(\n",
    "    file_name: str,\n",
    "    keep_all_columns: Optional[bool] = False,\n",
    "    keep_duplicated: Optional[bool] = True,\n",
    ") -> pd.DataFrame:\n",
    "    assert file_name.endswith(\".json\"), \"File must be a JSON file\"\n",
    "\n",
    "    df = pd.read_json(file_name)\n",
    "    for col in [\"date\", \"created_at\", \"updated_at\", \"deleted_at\"]:\n",
    "        df[col] = clean_datetime_serie(df[col])\n",
    "\n",
    "    for col in [\"created_by\", \"updated_by\", \"deleted_by\"]:\n",
    "        df[f\"{col}_id\"] = df[col].str[\"id\"]\n",
    "        df[f\"{col}_name\"] = df[col].str[\"first_name\"]\n",
    "\n",
    "    df[\"category_id\"] = df[\"category\"].str[\"id\"]\n",
    "    df[\"category_name\"] = df[\"category\"].str[\"name\"]\n",
    "\n",
    "    if not keep_all_columns:\n",
    "        df = df[EXPENSES_CLEANED_COLUMNS]\n",
    "\n",
    "    is_duplicated = is_duplicated_expense(df)\n",
    "    if keep_duplicated:\n",
    "        df[\"is_duplicated\"] = is_duplicated\n",
    "    else:\n",
    "        df = df[~is_duplicated]\n",
    "\n",
    "    df = df.rename(columns=lambda x: f\"expense_{x}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "expenses = read_expenses(\"raw/expenses.json\", keep_all_columns=False)\n",
    "expenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses[expenses.expense_is_duplicated].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f615116",
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses.groupby([\"expense_date\", \"expense_cost\"]).expense_id.count().pipe(\n",
    "    lambda s: s[s > 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40056162",
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses = expenses[~expenses.expense_is_duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1555b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def apply_transforms(\n",
    "    df: pd.DataFrame, transforms: Callable[[pd.DataFrame], pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    df = reduce(\n",
    "        lambda df, transform: transform(df),\n",
    "        transforms,\n",
    "        df,\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeac0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dict_column(column_name: str) -> Callable[[pd.DataFrame], pd.DataFrame]:\n",
    "    def inner(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        fill_na_values = lambda x: x if isinstance(x, dict) else {}\n",
    "        df[column_name] = df[column_name].apply(fill_na_values)\n",
    "        frame = (\n",
    "            df[column_name]\n",
    "            .apply(pd.Series)\n",
    "            .rename(columns=lambda x: f\"{column_name}_{x}\")\n",
    "        )\n",
    "        return frame.join(df.drop(column_name, axis=1))\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "TRANSACTION_CLEANED_COLUMNS = [\n",
    "    \"transaction_cost\",\n",
    "    \"transaction_currency_code\",\n",
    "    \"transaction_date\",\n",
    "    \"transaction_description\",\n",
    "    \"transaction_card_number\",\n",
    "    \"transaction_hash\",\n",
    "]\n",
    "\n",
    "\n",
    "def read_emails(\n",
    "    filename: str, keep_all_columns: Optional[bool] = False, keep_errored: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    assert filename.endswith(\".json\"), \"File must be a JSON file\"\n",
    "    df = pd.read_json(filename)\n",
    "    transforms = [expand_dict_column(\"email\"), expand_dict_column(\"transaction\")]\n",
    "    frame = apply_transforms(df, transforms)\n",
    "    for col in [\"transaction_date\", \"email_date\"]:\n",
    "        frame[col] = clean_datetime_serie(frame[col])\n",
    "\n",
    "    if not keep_errored:\n",
    "        frame = frame[frame[\"error\"].isna()]\n",
    "\n",
    "    if not keep_all_columns:\n",
    "        frame = frame[TRANSACTION_CLEANED_COLUMNS]\n",
    "    return frame\n",
    "\n",
    "\n",
    "transactions = read_emails(\"raw/emails.json\")\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cajero = transactions[\"transaction_description\"].str.contains(\n",
    "    \"con cargo a Cuenta ****4503\", regex=False\n",
    ")\n",
    "transactions[is_cajero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d885c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions[~is_cajero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b928f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[\"transaction_description\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e303e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.to_pickle(\"processed/transactions.pkl\")\n",
    "locations.to_file(\"processed/locations.geojson\", driver=\"GeoJSON\")\n",
    "expenses.to_pickle(\"processed/expenses.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8169ba",
   "metadata": {},
   "source": [
    "# Joined datasets with location and expenses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0a168",
   "metadata": {},
   "source": [
    "Cómo asociar transacciones con gastos\n",
    "1. Muy fácil: usar los campos hash. Asumir que las transacciones eliminadas significan \"gasto personal\"\n",
    "2. Asociar transacciones por día y con montos similares. Podemos asumir que cualquier transacción que no esté en Splitwise puede ser personal.\n",
    "\n",
    "Estudiar:\n",
    "1. ¿Existe alguna relación entre la descripción y el comerciante de la transacción? Es decir, ¿podemos crear una mejor descripción a partir del comerciante?\n",
    "2. ¿Cuál es la relación entre las ubicaciones y los gastos? Cuánto aporta en predecir que es un gasto personal/compartido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563dc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_transactions(\n",
    "    expenses_df: pd.DataFrame,\n",
    "    transactions_df: pd.DataFrame,\n",
    "    date_threshold: pd.Timedelta = pd.Timedelta(\"12h\"),\n",
    "    amount_threshold: float = 0.001,\n",
    ") -> pd.DataFrame:\n",
    "    # This is very slow, but it works\n",
    "    # Need to rework if you want to use all your emails and expenses\n",
    "\n",
    "    assert expenses_df[\"expense_id\"].is_unique, \"Expenses ids are not unique\"\n",
    "    assert transactions_df[\n",
    "        \"transaction_hash\"\n",
    "    ].is_unique, \"Transactions hashes are not unique\"\n",
    "\n",
    "    expenses = expenses_df.copy()\n",
    "    transactions = transactions_df.copy()\n",
    "\n",
    "    # Create empty lists to store match results\n",
    "    matches = []\n",
    "\n",
    "    # For each expense, find potential matching transactions\n",
    "    for _, expense in expenses.iterrows():\n",
    "        # Filter transactions by date range\n",
    "        date_min = expense[\"expense_date\"] - date_threshold\n",
    "        date_max = expense[\"expense_date\"] + date_threshold\n",
    "        date_filtered = transactions[\n",
    "            (transactions[\"transaction_date\"] >= date_min)\n",
    "            & (transactions[\"transaction_date\"] <= date_max)\n",
    "        ].copy()\n",
    "\n",
    "        if len(date_filtered) == 0:\n",
    "            continue\n",
    "\n",
    "        # Filter by amount similarity\n",
    "        amount = expense[\"expense_cost\"]\n",
    "        date_filtered[\"amount_diff\"] = (\n",
    "            abs(date_filtered[\"transaction_cost\"] - amount) / amount\n",
    "        )\n",
    "        potential_matches = date_filtered[\n",
    "            date_filtered[\"amount_diff\"] <= amount_threshold\n",
    "        ]\n",
    "        if len(potential_matches) > 0:\n",
    "\n",
    "            # Get the best match (smallest amount difference)\n",
    "            k = 1\n",
    "            top_matches = potential_matches.sort_values(\"amount_diff\").iloc[:k]\n",
    "            for _, best_match in top_matches.iterrows():\n",
    "                matches.append(\n",
    "                    {\n",
    "                        \"expense_id\": expense[\"expense_id\"],\n",
    "                        \"transaction_hash\": best_match[\"transaction_hash\"],\n",
    "                        \"amount_diff\": best_match[\"amount_diff\"],\n",
    "                        \"cost_diff\": abs(\n",
    "                            expense[\"expense_cost\"] - best_match[\"transaction_cost\"]\n",
    "                        ),\n",
    "                        \"expense_time_diff\": abs(\n",
    "                            expense[\"expense_date\"] - best_match[\"transaction_date\"]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "    matches = pd.DataFrame(matches)\n",
    "    assert matches.expense_id.is_unique, \"Expense ids are not unique in matches\"\n",
    "    # assert matches.transaction_hash.is_unique, \"Transaction hashes are not unique in matches\"\n",
    "    return matches\n",
    "\n",
    "\n",
    "for threshold in [0.1, 0.01, 0.001]:\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    matched_transactions = match_transactions(\n",
    "        expenses, transactions, amount_threshold=threshold\n",
    "    )\n",
    "    print(\n",
    "        f\"number of rows: {len(matched_transactions)}, max difference $ {matched_transactions['cost_diff'].max()} CLP\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_transactions = match_transactions(\n",
    "    expenses, transactions, amount_threshold=0.001\n",
    ")\n",
    "matched_transactions = matched_transactions.merge(\n",
    "    expenses, left_on=\"expense_id\", right_on=\"expense_id\", how=\"left\"\n",
    ").merge(\n",
    "    transactions,\n",
    "    left_on=\"transaction_hash\",\n",
    "    right_on=\"transaction_hash\",\n",
    "    how=\"right\",\n",
    ")\n",
    "matched_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b09deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_transactions[\n",
    "    matched_transactions.transaction_hash\n",
    "    == \"b03b6b7e3e2b8a70535488a40b6d30ec4aebd828a7a2161e4fb2de843ca2fc4e\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_locations(\n",
    "    transactions_df: pd.DataFrame,\n",
    "    locations_df: gpd.GeoDataFrame,\n",
    "    date_threshold: pd.Timedelta = pd.Timedelta(\"1m\"),\n",
    ") -> pd.DataFrame:\n",
    "    # This is very slow, but it works\n",
    "    # Need to rework if you want to use all your emails and expenses\n",
    "\n",
    "    assert transactions_df[\n",
    "        \"transaction_hash\"\n",
    "    ].is_unique, \"Transactions hashes are not unique\"\n",
    "\n",
    "    transactions = transactions_df.copy()\n",
    "    locations = locations_df.copy()\n",
    "\n",
    "    # Create empty lists to store match results\n",
    "    matches = []\n",
    "\n",
    "    # For each transaction, find potential matching locations\n",
    "    for _, transaction in transactions.iterrows():\n",
    "        # Filter locations by date range\n",
    "        date_min = transaction[\"transaction_date\"] - pd.Timedelta(date_threshold)\n",
    "        date_max = transaction[\"transaction_date\"] + pd.Timedelta(date_threshold)\n",
    "        date_filtered = locations[\n",
    "            (locations[\"location_date\"] >= date_min)\n",
    "            & (locations[\"location_date\"] <= date_max)\n",
    "        ].copy()\n",
    "        date_filtered[\"time_diff\"] = abs(\n",
    "            date_filtered[\"location_date\"] - transaction[\"transaction_date\"]\n",
    "        )\n",
    "\n",
    "        if len(date_filtered) == 0:\n",
    "            continue\n",
    "\n",
    "        potential_matches = date_filtered\n",
    "        if len(potential_matches) > 0:\n",
    "            k = 1\n",
    "            top_matches = potential_matches.sort_values(\"time_diff\").iloc[:k]\n",
    "            for _, best_match in top_matches.iterrows():\n",
    "                matches.append(\n",
    "                    {\n",
    "                        \"transaction_hash\": transaction[\"transaction_hash\"],\n",
    "                        \"location_id\": best_match[\"location_id\"],\n",
    "                        \"location_time_diff\": best_match[\"time_diff\"],\n",
    "                    }\n",
    "                )\n",
    "    matches = pd.DataFrame(matches)\n",
    "    assert (\n",
    "        matches.transaction_hash.is_unique\n",
    "    ), \"Transaction hashes are not unique in matches\"\n",
    "    # assert matches.location_id.is_unique, \"Location ids are not unique in matches\"\n",
    "    return matches\n",
    "\n",
    "\n",
    "matched_locs = match_locations(transactions, locations, pd.Timedelta(\"5m\"))\n",
    "matched_transactions_locs = matched_locs.merge(\n",
    "    locations, left_on=\"location_id\", right_on=\"location_id\", how=\"left\"\n",
    ").merge(\n",
    "    matched_transactions,\n",
    "    left_on=\"transaction_hash\",\n",
    "    right_on=\"transaction_hash\",\n",
    "    how=\"right\",\n",
    ")\n",
    "matched_transactions_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e24f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    transactions.shape[0] == matched_transactions_locs.shape[0]\n",
    "), \"Number of transactions do not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad075314",
   "metadata": {},
   "source": [
    "# Label classes is_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1953fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_transactions_locs[\"expense_deleted_by_name\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65630f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_transactions_locs[\"is_shared_feedback\"] = (\n",
    "    matched_transactions_locs[\"expense_deleted_by_name\"] == \"Diego\"\n",
    ")  # manual feedback in the app - when the user deletes the expense\n",
    "matched_transactions_locs[\"is_shared\"] = (\n",
    "    matched_transactions_locs[\"expense_id\"].notnull()\n",
    "    | matched_transactions_locs[\"is_shared_feedback\"]\n",
    ")\n",
    "matched_transactions_locs[matched_transactions_locs[\"is_shared_feedback\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb489458",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_transactions_locs.to_pickle(\n",
    "    \"processed/matched_transactions_locs.pkl\"\n",
    ")  # Save the matched transactions with locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42aeed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
